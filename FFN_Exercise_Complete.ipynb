{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Build a Feed-Forward Neural Network\n",
    "\n",
    "This notebook outlines two different neural network simulations. \n",
    "\n",
    "In the first simulation we will train a **2-layer feed-forward network** on different logic gates (AND/OR/XOR). We will train the network using the **delta-rule** and the network learn both the *AND* rule, as well as the *OR* rule. Finally, we will investigate the limitations of this network by training it on the *XOR* rule.\n",
    "\n",
    "In the second simulation we will train a **3-layer neural network** with a set of non-linear hidden units. We will use **backpropagation** to train the network and show that it is able to learn the XOR rule. Finally, we will make an attempt to investigate the network's learned weights in order to understand it's learned solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements\n",
    "\n",
    "We need numpy in order to do math conveniently in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Environment\n",
    "\n",
    "Before we build the network, define what the network will be tasked with. All tasks provided to the network are expressed as patterns in the training environment. There are two types of patterns. **Input patterns** are provided to the input layer of the network. **training patterns** represent the 'correct' response to the input patterns. \n",
    "\n",
    "We will define 3 different training environments that are based on the *boolean functions* AND, OR and XOR. The input patterns for each of the environments is the same. Each input pattern consists of 2 bits that are passed to the network. That is, the network takes as input two values that can be either 0 or 1. Each of the three boolean functions (AND/OR/XOR) defines how these two inputs are combined to a single binary output value. \n",
    "\n",
    "Boolean functions can be expressed in a *truth table*. Every row in a truth table represents a single input pattern with it's corresponding output pattern. Below you find the truth table for each of the boolean functions that we use for training. \n",
    "\n",
    "\n",
    "<br>\n",
    "<center>**AND**</center>\n",
    "\n",
    "| Input 1 | Input 2 | Output |\n",
    "|---------|---------|--------|\n",
    "| 0       | 0       | 0      |\n",
    "| 0       | 1       | 0      |\n",
    "| 1       | 0       | 0      |\n",
    "| 1       | 1       | 1      |\n",
    "\n",
    "<br>\n",
    "<center>**OR**</center>\n",
    "\n",
    "| Input 1 | Input 2 | Output |\n",
    "|---------|---------|--------|\n",
    "| 0       | 0       | 0      |\n",
    "| 0       | 1       | 1      |\n",
    "| 1       | 0       | 1      |\n",
    "| 1       | 1       | 1      |\n",
    "\n",
    "\n",
    "<br>\n",
    "<center>**XOR**</center>\n",
    "\n",
    "| Input 1 | Input 2 | Output |\n",
    "|---------|---------|--------|\n",
    "| 0       | 0       | 0      |\n",
    "| 0       | 1       | 1      |\n",
    "| 1       | 0       | 1      |\n",
    "| 1       | 1       | 0      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define input patterns (same across task environments)\n",
    "input_patterns = np.array([[ 0.,  0.],\n",
    "                           [ 0.,  1.],\n",
    "                           [ 1.,  0.],\n",
    "                           [ 1.,  1.]])\n",
    "\n",
    "# define output patterns for AND rule\n",
    "output_patterns_AND = np.array([[ 0.],\n",
    "                                [ 0.],\n",
    "                                [ 0.],\n",
    "                                [ 1.]])\n",
    "\n",
    "# define output patterns for OR rule\n",
    "output_patterns_OR = np.array([[ 0.],\n",
    "                               [ 1.],\n",
    "                               [ 1.],\n",
    "                               [ 1.]])\n",
    "\n",
    "output_patterns_XOR = np.array([[ 0.],\n",
    "                                [ 1.],\n",
    "                                [ 1.],\n",
    "                                [ 0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Layer Neural Network & Delta-Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Training\n",
    "\n",
    "Let's build the function of the actual network. Our desired network looks like this:\n",
    "\n",
    "<img src=\"2LNetwork.png\" alt=\"2 Layer Network\" style=\"width: 250px;\"/>\n",
    "\n",
    "The input layer of the network encompasses two units. These two input units project to the unit of the output layer. Your task is to fill in the code in the function `train2LayerNetwork` that is used to train the network. The function will perform the following computations (steps marked in *italic* are the focus of this exercise)\n",
    "\n",
    "1) Initialization: This step involves initializing layers and weights\n",
    "\n",
    "*2) Feedforward Pass*: In this step we will compute the network's activity based on it's input pattern and it's weights.\n",
    "\n",
    "*3) Backpard Pass*: In this step we will adjust the weights of the network based on the produced output patterns of the network and the feedback provided by the training patterns\n",
    "\n",
    "Steps 2) and 3) will be performed for each input pattern in each training iteration. The following sections discuss each step in detail.\n",
    "\n",
    "#### Forward Pass ####\n",
    "\n",
    "In the forward pass we will propagate activity through the network, layer by layer. In feedforward networks, we already know the activity of the input layer as it corresponds to the input pattern. We will therefore begin with the computing the activity of the units in the second layer. Let's say that the second layer has $N$ units. Let $y_j$\n",
    "be the activation of a unit in the second layer that we want to compute where $j \\in \\{1,...,N\\}$. The activation of a unit is a function of its net input. We are usually interested in differtiable, non-monotonic functions, such as the sigmoidal activation function:\n",
    "\n",
    "\\begin{equation}\n",
    "y_j = \\frac{1}{1+e^{-net_{y_j}}}\n",
    "\\end{equation}\n",
    "\n",
    "This function makes sure that the activation of a unit is bound between 0 and 1. However, in order to compute the activity $y_i$ we need to know the net input of a unit. The net input of unit $y_i$ is simply the sum of the activity of the sending units in the previous layer, weighted by their projection weights. Let's that the sending, input layer has $M$ units. Then the net input $y_i$ of a unit in the receiving layer corresponds to\n",
    "\n",
    "\\begin{equation}\n",
    "net_{y_j} = \\sum_{i=1}^M x_i w_{j,i} \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where $x_i$ corresponds to the activity of input unit $i$ and $w_{j,i}$ corresponds to the weight of input unit $i$ to unit $j$ in the second layer. \n",
    "\n",
    "In a network with more than two layers, one would then proceed with computing the activation of the units in the third layer. This is done the same way as with the second layer: The activity of each unit in the third layer is some activation function of its net input. The net input is the weighted sum of the activities of the second layer where the weights correspond to the projection weights from the second to the third layer. In the feedforward pass one can apply this procedure layer by layer until the activity of the final (output) layer is computed. \n",
    "\n",
    "Now that we know the general rule to compute the activity of a neural network we can apply this to our example above. Say we want to compute the networks activity for the input pattern $[0,1]$. Since this is a very simple network, all we need to do is compute the activity of the unit $y_1$ in the second (output) layer. Let's assume that the network has the weights\n",
    "\n",
    "$w_{1,1} = -0.5$ \n",
    "\n",
    "$w_{1,2} = 2$.\n",
    "\n",
    "Since the input pattern is $[0,1]$, the the input layer units take on the following values:\n",
    "\n",
    "$x_1 = 1$\n",
    "\n",
    "$x_2 = 0$.\n",
    "\n",
    "Now we can compute the net input of $y_1$:\n",
    "\n",
    "$net_{y_1} = w_{1,1} x_1 + w_{1,2} x_2 = 0.5 * 1 + 2 * 0 = -0.5$\n",
    "\n",
    "Finally we can compute the activation of the unit for the input $[0,1]$:\n",
    "\n",
    "$y_1 = \\frac{1}{1+e^{-net_{y_1}}} = \\frac{1}{1+e^{-(-0.5)}} = 0.3775406688$\n",
    "\n",
    "What does this output mean? It means that the network implements a function that maps the input pattern $[0,1]$ to the value $0.3775406688$. What if we wanted the network to implement a different function like the OR-rule? In this case we would prefer the network to produce an output more close to $1$. We could do this by changing the weights of the network in a smart way. What if we don't want to think about which weights to choose? What if we could make the network *learn* the right weights itself? We will discuss how this can be done for our two-layer network in the next section.\n",
    "\n",
    "\n",
    "#### Backward Pass ####\n",
    "\n",
    "Our goal is to teach the network a particular function (e.g. the OR-rule). That is, we want it to produce the correct output (e.g. $[1]$) for a given input pattern (e.g. $[0,1]$). Let's assume that we already have a network with some  weights, like the ones from the feedforward pass. We know from the forward pass in the previous section that its output for the input pattern $[0,1]$ isn't very close to what we want $[1]$. So let's teach the network by providing it some feedback about how well it did. We will do this by computing the error $E_j$ of output unit $j$. One way to compute the error is by taking the squared difference between the output of the network and the correct training pattern for a given output unit $j$:\n",
    "\n",
    "\\begin{equation}\n",
    "E_{y_j} = 0.5(y_j - t_j)^2\n",
    "\\end{equation}\n",
    "\n",
    "where $t_j$ is the correct training output for unit $j$. Note that the squared error is scaled by 0. This is done in order to make computations work out nicer below. \n",
    "\n",
    "We can then use the error as a feedback signal to adjust its weights. We know that the error is a function of the weights of the network, e.g. a function of $w_{1,1}$. Let's assume that this <font color=\"#C00000\">error function of $w_{1,1}$ </font> looks like the solid red line in the following plot:\n",
    "\n",
    "<img src=\"ErrorSurface.png\" alt=\"2 Layer Network\" style=\"width: 400px;\"/>\n",
    "\n",
    "where <font color=\"#000000\"> $w_{1,1}^t$ </font> is the current weight of the network  at time step $t$ (e.g. -0.5) and <font color=\"#2F5597\">$w_{1,1}^*$</font> is the <font color=\"#2F5597\">optimal weight</font> of the network that provides the minimum error. In order to minimize the error we want to change our current weight <font color=\"#000000\"> $w_{1,1}^t$ </font> so that it becomes <font color=\"#2F5597\">$w_{1,1}^*$</font>. \n",
    "\n",
    "Perhaps we can compute <font color=\"#2F5597\">$w_{1,1}^*$</font> directly by finding the minimum of the <font color=\"#C00000\">error function</font>? Well, the problem is that we don't know the error function. On top of that, the error also depends on the other weight $w_{1,2}^t$. However, what if we had knowledge about the <font color=\"7030A0\">derivative of the error with respect to $w_{1,1}^t$ given the current weights, i.e. $\\frac{\\partial E_{y_1}^t}{\\partial w_{1,1}^t}$</font>? Then we could use the slope of the derivitave as an indicator in which direction we would have to change the weight $w_{1,1}^t$. In the exampple above, the derivative of the error function has a negative slope. This means, that, in order to minimize the error, we would have to increase the weight $w_{1,1}^t$ by some small amount <font color=\"#00B050\">$\\Delta w_{1,1}^t$</font>. That is, our <font color=\"C55A11\">weight for the next time step $w_{1,1}^{t+1}$</font> is computed as\n",
    "\n",
    "\\begin{equation}\n",
    "w_{1,1}^{t+1} = \\underbrace{w_{1,1}^t}_\\text{current weight} + \\underbrace{\\Delta w_{1,1}^t}_\\text{weight change}\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta w_{1,1}^t = - \\alpha \\underbrace{\\frac{\\partial E_{y_1}^t}{\\partial w_{1,1}^t}}_\\text{slope error of derivative}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha$ is a constant that defines the step size of the weight change (how much we want to change weight in the direction of the derivative). We call $\\alpha$ the *learning rate*. \n",
    "\n",
    "We still need to figure out how to compute <font color=\"7030A0\"> $\\frac{\\partial E_{y_1}^t}{\\partial w_{1,1}^t}$ </font>, that is, the partial derivative of the error with respect to $w_{1,1}^t$. We know that the error of unit $j$ is a function of its output activity $y_j$, the unit's activity $y_j$ is a function of its net input $net_{y_1}$, and it's net input is a function of the current weight $w_{1,1}^t$. Knowing this, we can apply the chain rule:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E_{y_1}^t}{\\partial w_{1,1}^t} = \\underbrace{\\frac{\\partial E_{y_1}^t}{\\partial y_1}}_\\text{derivative 1} \\quad\n",
    "\\underbrace{\\frac{\\partial y_1}{\\partial net_{y_1}}}_\\text{derivative 2} \\quad\n",
    "\\underbrace{\\frac{\\partial net_{y_1}}{\\partial w_{1,1}^t}}_\\text{derivative 3}\n",
    "\\end{equation}\n",
    "\n",
    "The first partial derivative $\\frac{\\partial E_{y_1}^t}{\\partial y_1}$ is easy to compute:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E_{y_1}^t}{\\partial y_1} =  \\frac{\\partial 0.5(y_1 - t_1)^2}{\\partial y_1} = (y_1 - t_1)\n",
    "\\end{equation}\n",
    "\n",
    "The second partial derivative $\\frac{\\partial y_1}{\\partial net_{y_1}}$ is a bit more complicated since we are dealing with a sigmoidal activation function $y_1 = 1/(1+e^{-net_{y_1}})$. However, it turns out that it's derivate can be simply computed as a function of $y_1$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial y_1}{\\partial net_{y_1}} = y_1 (1 - y_1)\n",
    "\\end{equation}\n",
    "\n",
    "Finally, we compute the third partial derivative $\\frac{\\partial net_{y_1}}{\\partial w_{1,1}^t}$ as \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial net_{y_1}}{\\partial w_{1,1}^t} =  x_1\n",
    "\\end{equation}\n",
    "\n",
    "Now let's put all pieces together in order to compute the final weight change for $w_{1,1}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta w_{1,1}^t = - \\alpha (y_1 - t_1) y_1 (1 - y_1) x_1\n",
    "\\end{equation}\n",
    "\n",
    "A similar update rule can be applied for the other weight $w_{1,2}$.\n",
    "\n",
    "#### Additional Remarks ####\n",
    "Note that if we use a linear activation function, e.g. $y_j = m net_{y_j} + n$, then the weight change reduces to \n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta w_{j,i}^t = - \\alpha (y_j - t_j) x_i\n",
    "\\end{equation}\n",
    "\n",
    "where the constant $m$ gets absorbed in the learning rate. This weight update rule is termed the delta rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "\n",
    "The function `train2LayerNetwork` initializes a simple 2-layer neural network with 2 input units and 1 output unit (like in the example above). Your task is to fill in the missing code for the computation of its output (forward pass), as well as the missing code for network training (backward pass). The missing code is marked with '`...`'.\n",
    "\n",
    "Once you completed the code, run the simulation below. The simulation will output for each learning epoch the mean squared error across all training patterns. Try to train the network on the two different training patterns (AND, OR). Why do you think does the network not learn the task as well? You may set the debug variable to true for a more detailed output of the weight adjustments for each pattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train2LayerNetwork(input_patterns, output_patterns, learning_rate, MSE_threshold, num_epochs, debug=False):\n",
    "\n",
    "    ### network initialization ###\n",
    "\n",
    "    # let's define the number input and output units as a function of the dimension of the input and output patterns respectively\n",
    "    NInputUnits = input_patterns.shape[1]\n",
    "    NOutputUnits = output_patterns.shape[1]\n",
    "\n",
    "    # let's also log the error of the network\n",
    "    MSE_log = np.zeros((1, num_epochs))\n",
    "\n",
    "    # we will also randomly initialize the weights between the input and output layer, \n",
    "    # as well as the bias weights to the output layer\n",
    "    # the weight matrix will have as many as number of rows as there are units in the output layer\n",
    "    # and as many number of columns as there are units in the input layer\n",
    "    # weights will be initialized with small random values, uniformly sampled between 0 and 0.1\n",
    "    W_yx = np.random.uniform(0, 0.1,(NOutputUnits, NInputUnits))\n",
    "    W_ybias = np.random.uniform(0, 0.1,(NOutputUnits, 1))\n",
    "    \n",
    "    ### network training ### \n",
    "\n",
    "    # the network will be trained in epochs.\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # initialize mean squared error log for all patterns\n",
    "        MSE_patterns = np.zeros((output_patterns.shape[0],))\n",
    "\n",
    "        # within each training epoch, we will loop through every training pattern. \n",
    "        for pattern in range(input_patterns.shape[0]):\n",
    "\n",
    "            # FORWARD PASS #\n",
    "\n",
    "            # assign values to input layer\n",
    "            x = input_patterns[pattern,]\n",
    "\n",
    "            # compute net input of output layer\n",
    "            y_net = np.dot(W_yx, x.T) + 1 * W_ybias\n",
    "\n",
    "            # compute activation of output layer using sigmoidal activation function\n",
    "            y = 1/(np.exp(- (y_net))+1)\n",
    "\n",
    "            # ERROR BACKPROPAGATION #\n",
    "\n",
    "            # compute the mean squared error of the output with respect to the correct training pattern\n",
    "            MSE_patterns[pattern] = (y-output_patterns[pattern,])**2\n",
    "\n",
    "            # compute derivative of the error with respect to the output unit activation\n",
    "            dError_dAct = (y-output_patterns[pattern,])\n",
    "\n",
    "            # compute derivative of output unit activation with respect to it's net input. \n",
    "            # Note that the derivative of a sigmoidal function y(x) = 1/(1+exp(-x)) \n",
    "            # is dy = y(x) * (1-y(x))\n",
    "            dAct_dNet = y * (1-y)\n",
    "\n",
    "            # compute the derivative of the net input of the output layer with respect to it's weights to the input layer\n",
    "            dNet_dW_yx = x\n",
    "            \n",
    "            # compute the derivative of the net input of the output layer with respect to it's weights to the bias unit\n",
    "            dNet_dW_ybias = 1\n",
    "\n",
    "            # compute weight adjustment\n",
    "            delta_W_yx = dError_dAct * dAct_dNet * dNet_dW_yx;\n",
    "            delta_W_ybias = dError_dAct * dAct_dNet * dNet_dW_ybias;\n",
    "\n",
    "            # For debugging\n",
    "            if(debug):\n",
    "                print('----------')\n",
    "                print('pattern:')\n",
    "                print(x)\n",
    "                print('weights:')\n",
    "                print(W_yx)\n",
    "                print('output:')\n",
    "                print(y)\n",
    "                print('MSE:')\n",
    "                print(MSE_patterns[pattern])\n",
    "                print('-')\n",
    "                print('output pattern:')\n",
    "                print(output_patterns[pattern,])\n",
    "                print('dError:')\n",
    "                print(dError_dAct)\n",
    "                print('dAct_dNet:')\n",
    "                print(dAct_dNet)\n",
    "                print('weight adjustment:')\n",
    "                print(- learning_rate * delta_W_yx)\n",
    "\n",
    "            # adjust weights based on learning rate\n",
    "            W_yx = W_yx - learning_rate * delta_W_yx\n",
    "            W_ybias = W_ybias - learning_rate * delta_W_ybias\n",
    "\n",
    "\n",
    "        # log mean squared error for current epoch \n",
    "        MSE_log[0,epoch] = np.sum(MSE_patterns)/MSE_patterns.size\n",
    "\n",
    "        # print mean squared error\n",
    "        if epoch == 0:\n",
    "            print('Training MSE:')\n",
    "        print(MSE_log[0,epoch])\n",
    "        \n",
    "        # break if we error threshold is reached\n",
    "        if MSE_log[0,epoch] < MSE_threshold:\n",
    "            break\n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Simulation\n",
    "\n",
    "We will start with defining critical simulation parameters:\n",
    "- `learning_rate` corresponds to the stepsize for each weight change\n",
    "- `MSE_threshold` defines the mean-squared error at which training is stopped\n",
    "- `max_num_epochs` corresponds to the maximum number of training iterations (in case MSE_threshold is not reached).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:\n",
      "0.256930970518\n",
      "0.246846104977\n",
      "0.238148551492\n",
      "0.230600943688\n",
      "0.223999907416\n",
      "0.218175743215\n",
      "0.212989094443\n",
      "0.208326576898\n",
      "0.204096389057\n",
      "0.200224356979\n",
      "0.196650564164\n",
      "0.193326568218\n",
      "0.190213142868\n",
      "0.187278464137\n",
      "0.184496660265\n",
      "0.18184665413\n",
      "0.17931123852\n",
      "0.176876335878\n",
      "0.174530403994\n",
      "0.172263957305\n",
      "0.170069180035\n",
      "0.167939612612\n",
      "0.165869896808\n",
      "0.163855568208\n",
      "0.161892887009\n",
      "0.159978700021\n",
      "0.158110328154\n",
      "0.156285474817\n",
      "0.15450215149\n",
      "0.152758617426\n",
      "0.151053330975\n",
      "0.149384910461\n",
      "0.147752102879\n",
      "0.146153758995\n",
      "0.144588813634\n",
      "0.143056270192\n",
      "0.14155518852\n",
      "0.140084675527\n",
      "0.138643877925\n",
      "0.137231976663\n",
      "0.135848182687\n",
      "0.134491733722\n",
      "0.133161891828\n",
      "0.131857941557\n",
      "0.13057918855\n",
      "0.129324958461\n",
      "0.128094596122\n",
      "0.126887464872\n",
      "0.12570294601\n",
      "0.124540438326\n",
      "0.123399357687\n",
      "0.122279136653\n",
      "0.121179224112\n",
      "0.120099084929\n",
      "0.119038199595\n",
      "0.117996063877\n",
      "0.116972188463\n",
      "0.115966098614\n",
      "0.114977333797\n",
      "0.114005447334\n",
      "0.113050006034\n",
      "0.112110589838\n",
      "0.111186791451\n",
      "0.11027821599\n",
      "0.109384480623\n",
      "0.108505214219\n",
      "0.107640056999\n",
      "0.106788660193\n",
      "0.105950685704\n",
      "0.105125805776\n",
      "0.104313702671\n",
      "0.10351406835\n",
      "0.102726604169\n",
      "0.101951020569\n",
      "0.10118703679\n",
      "0.100434380577\n",
      "0.0996927879079\n",
      "0.0989620027186\n",
      "0.098241776642\n",
      "0.0975318687522\n",
      "0.0968320453176\n",
      "0.0961420795613\n",
      "0.0954617514285\n",
      "0.0947908473619\n",
      "0.0941291600844\n",
      "0.0934764883884\n",
      "0.0928326369324\n",
      "0.0921974160444\n",
      "0.0915706415317\n",
      "0.0909521344974\n",
      "0.0903417211629\n",
      "0.0897392326968\n",
      "0.0891445050493\n",
      "0.088557378793\n",
      "0.0879776989685\n",
      "0.087405314936\n",
      "0.0868400802319\n",
      "0.0862818524305\n",
      "0.0857304930104\n",
      "0.085185867226\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.3\n",
    "MSE_threshold = 0.05\n",
    "num_epochs = 100\n",
    "\n",
    "train2LayerNetwork(input_patterns, output_patterns_AND, learning_rate, MSE_threshold, num_epochs, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Role of Bias Units\n",
    "\n",
    "All of the logical boolean functions have something in common: They require to map the inputs $[0,0]$ to the output $[0]$. Can our neural network actually learn this mapping? Let's compute the network's output in response to the input pattern $x_1=0, x_2=0$ for any given weights $w_{1,1}, w_{1,2}$:\n",
    "\n",
    "$net_{y_1} = w_{1,1} x_1 + w_{1,2} x_2 = 0$\n",
    "\n",
    "$y_1 = \\frac{1}{1+e^{(-net_{y_1})}} = \\frac{1}{1+e^{(-0)}} = 0.5$\n",
    "\n",
    "That is, the network will not be able to produce an output of 0 for any given weights $w_{1,1}, w_{1,2}$. This is because the inputs $x_1=0, x_2=0$ lead to $net_{y_1} = 0$, which in turn yields an activation of $y_1 = 0.5$ due to the sigmoidal activation function:\n",
    "\n",
    "<img src=\"BiasEffect.png\" alt=\"effect of bias on sigmoidal activation function\" style=\"width: 250px;\"/>\n",
    "\n",
    "What if we could shift the sigmoidal activation function to the right? Then a net input of 0 would yield an activation close to 0. Shifting the sigmoidal activation function corresponds to adding a negative bias term to the net input, i.e.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "net_{y_1} = w_{1,1} x_1 + w_{1,2} x_2 \\underbrace{-4}_\\text{bias term}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "y_1 = \\frac{1}{1+e^{(-(w_{1,1} x_1 + w_{1,2} x_2 - 2))}} = \\frac{1}{1+e^{(4)}} = 0.0180\n",
    "\\end{equation}\n",
    "\n",
    "Since the sigmmoidal activation function is bounded between 0 and 1, it's activation will never exactly 0 or 1. However, we can get it as close to 0 as possible in order to minimize the error.\n",
    "\n",
    "What if we could teach the network to find the right bias to the output unit $y_j$? As shown above, the bias is just an additional term in the net input of a unit. We can therefore treat it as a separate input unit with value $b_{y_j}$ with a weight $w_{j}$ that projects to the output unit $y_j$:\n",
    "\n",
    "<img src=\"2LNetworkBias.png\" alt=\"2-layer network with bias unit\" style=\"width: 400px;\"/>\n",
    "\n",
    "The net input of $y_j$ then amounts to\n",
    "\n",
    "\\begin{equation}\n",
    "net_{y_1} = w_{1,1} x_1 + w_{1,2} x_2 + \\underbrace{b_{y_j} w_{j}}_\\text{bias term}\n",
    "\\end{equation}\n",
    "\n",
    "Now that we have expressed the bias term as another unit with a projection weight to $y_j$, we scan imply set the bias input unit $b_{y_j} = 1$ for every input pattern and let the network learn its weight $w_{j,b}$ in order to find the optimal bias of the network:\n",
    "\n",
    "\\begin{equation}\n",
    "w_{j,b}^{t+1} = \\underbrace{w_{j,b}^t}_\\text{current weight} + \\underbrace{\\Delta w_{j,b}^t}_\\text{weight change}\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta w_{j,b}^t = - \\alpha \\underbrace{\\frac{\\partial E_{y_1}^t}{\\partial w_{j,b}^t}}_\\text{slope error of derivative}\n",
    "\\end{equation}\n",
    "\n",
    "Note that any unit (except input units) in the network can have a bias unit. At is indeed usually the case that every unit of a layer in a network (except input layers) have their own bias units.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "\n",
    "Modify `train2LayerNetwork` to implement a bias unit on the output unit, as well as a mechanism for learning the weight of that bias unit. Check if implementing a bias unit imporves learning on the AND & OR rule. \n",
    "\n",
    "Finally, test the network's ability to learn the XOR rule. Why does it still have trouble learning the rule? How could we modify the network even further in order to make it learn the XOR rule?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three-Layer Neural Network & Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train3LayerNetwork(input_patterns, output_patterns, learning_rate, MSE_threshold, num_epochs, debug=False):\n",
    "\n",
    "    ### network initialization ###\n",
    "\n",
    "    # let's define the number input and output units as a function of the dimension of the input and output patterns respectively\n",
    "    NInputUnits = input_patterns.shape[1]\n",
    "    NOutputUnits = output_patterns.shape[1]\n",
    "    # set number of hidden units to 3\n",
    "    NHiddenUnits = 3\n",
    "\n",
    "    # no we can intialize the two layers of the network...\n",
    "    x = np.zeros((1, NInputUnits)) # input layer\n",
    "    h = np.zeros((1, NHiddenUnits)) # hidden layer\n",
    "    y = np.zeros((1, NOutputUnits)) # output layer\n",
    "\n",
    "    # let's also log the error of the network\n",
    "    MSE_log = np.zeros((1, num_epochs))\n",
    "\n",
    "    # initialize weights from input layer to the hidden layer\n",
    "    W_hx = np.random.uniform(0, 0.1,(h.shape[1],x.shape[1]))\n",
    "    \n",
    "    # initialize weights from hidden layer to the output layer\n",
    "    W_yh = np.random.uniform(0, 0.1,(y.shape[1],h.shape[1]))\n",
    "    \n",
    "    # initialize weights from bias unit to hidden layer and output layer\n",
    "    W_hbias = np.random.uniform(0, 0.1,(h.shape[1],1))\n",
    "    W_ybias = np.random.uniform(0, 0.1,(y.shape[1],1))\n",
    "\n",
    "    ### network training ### \n",
    "\n",
    "    # the network will be trained in epochs.\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # initialize mean squared error log for all patterns\n",
    "        MSE_patterns = np.zeros((output_patterns.shape[0],))\n",
    "\n",
    "        # shuffle input patterns\n",
    "        # np.random.shuffle(input_patterns)\n",
    "\n",
    "        # within each training epoch, we will loop through every training pattern. \n",
    "        for pattern in range(input_patterns.shape[0]):\n",
    "\n",
    "            # FORWARD PASS #\n",
    "\n",
    "            # assign values to input layer\n",
    "            x = input_patterns[pattern,]\n",
    "\n",
    "            # compute net input of hidden layer\n",
    "            hbias = 1 * W_hbias\n",
    "            h_net = np.dot(W_hx, x.transpose()) + hbias.transpose()\n",
    "            \n",
    "            # compute activation of hidden layer using sigmoidal activation function\n",
    "            h = 1/(np.exp(- (h_net))+1)\n",
    "            \n",
    "            # compute net input of output layer\n",
    "            ybias = 1 * W_ybias\n",
    "            y_net = np.dot(W_yh, h.transpose()) + ybias.transpose()\n",
    "            \n",
    "            # compute activation of output layer using sigmoidal activation function\n",
    "            y = 1/(np.exp(- (y_net))+1)\n",
    "\n",
    "            # ERROR BACKPROPAGATION #\n",
    "\n",
    "            # compute the mean squared error of the output with respect to the correct training pattern\n",
    "            MSE_patterns[pattern] = (y-output_patterns[pattern,])**2\n",
    "\n",
    "            # compute derivative of the error with respect to the output unit activation\n",
    "            dError_dAct = (y-output_patterns[pattern,])\n",
    "\n",
    "            # compute derivative of output unit activation with respect to it's net input. \n",
    "            # Note that the derivative of a sigmoidal function y(x) = 1/(1+exp(-x)) \n",
    "            # is dy = y(x) * (1-y(x))\n",
    "            dAct_dNet = y * (1-y)\n",
    "            \n",
    "            # compute delta over output units\n",
    "            delta_y = dError_dAct * dAct_dNet\n",
    "\n",
    "            # compute the derivative of the net input of the output layer with respect to it's weights to the input layer\n",
    "            dNet_dW_yh = h\n",
    "            \n",
    "            # compute the derivative of the net input of the output layer with respect to it's weights to the bias unit\n",
    "            dNet_dW_ybias = 1\n",
    "            dNet_dW_hbias = 1\n",
    "\n",
    "            # compute weight adjustments from hidden to output layer\n",
    "            delta_W_yh = delta_y * dNet_dW_yh;\n",
    "            delta_W_ybias = delta_y * dNet_dW_ybias;\n",
    "            \n",
    "            dNet_y_dh = W_yh\n",
    "            dh_dNet = h * (1-h)\n",
    "            dNet_h_dx = x\n",
    "        \n",
    "            # compute derivative of net input of output layer with respect to activation of hidden layer units\n",
    "            delta_h = delta_y * np.multiply(dNet_y_dh, dh_dNet).transpose()\n",
    "            delta_W_hx = np.dot(delta_h, np.reshape([0,1], (1,2)))\n",
    "        \n",
    "            delta_W_hbias = delta_h * dNet_dW_hbias\n",
    "            \n",
    "            # adjust weights to output layer\n",
    "            W_yh = W_yh - learning_rate * delta_W_yh\n",
    "            W_ybias = W_ybias - learning_rate * delta_W_ybias\n",
    "            \n",
    "            # adjust weights to hidden layer\n",
    "            W_hx = W_hx - learning_rate * delta_W_hx\n",
    "            W_hbias = W_hbias - learning_rate * delta_W_hbias\n",
    "\n",
    "        # log mean squared error for current epoch \n",
    "        MSE_log[0,epoch] = np.sum(MSE_patterns)/MSE_patterns.size\n",
    "\n",
    "        # print mean squared error\n",
    "        print(MSE_log[0,epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.259337616472\n",
      "0.259206538434\n",
      "0.259106102524\n",
      "0.25902906691\n",
      "0.258969886823\n",
      "0.258924322611\n",
      "0.258889135897\n",
      "0.258861855081\n",
      "0.258840594958\n",
      "0.258823918298\n",
      "0.25881072983\n",
      "0.258800195157\n",
      "0.258791678805\n",
      "0.258784696928\n",
      "0.258778881222\n",
      "0.258773951403\n",
      "0.258769694236\n",
      "0.258765947536\n",
      "0.258762587994\n",
      "0.258759521888\n",
      "0.258756678017\n",
      "0.258754002316\n",
      "0.258751453755\n",
      "0.258749001217\n",
      "0.258746621119\n",
      "0.258744295608\n",
      "0.258742011181\n",
      "0.258739757636\n",
      "0.258737527278\n",
      "0.258735314314\n",
      "0.258733114387\n",
      "0.258730924233\n",
      "0.258728741409\n",
      "0.258726564097\n",
      "0.258724390945\n",
      "0.258722220956\n",
      "0.258720053397\n",
      "0.258717887734\n",
      "0.25871572358\n",
      "0.258713560661\n",
      "0.258711398782\n",
      "0.25870923781\n",
      "0.258707077656\n",
      "0.258704918262\n",
      "0.258702759595\n",
      "0.258700601637\n",
      "0.258698444382\n",
      "0.258696287831\n",
      "0.258694131991\n",
      "0.258691976874\n",
      "0.25868982249\n",
      "0.258687668852\n",
      "0.258685515975\n",
      "0.25868336387\n",
      "0.258681212551\n",
      "0.25867906203\n",
      "0.258676912316\n",
      "0.25867476342\n",
      "0.258672615352\n",
      "0.258670468121\n",
      "0.258668321733\n",
      "0.258666176195\n",
      "0.258664031515\n",
      "0.258661887698\n",
      "0.258659744748\n",
      "0.258657602671\n",
      "0.258655461471\n",
      "0.258653321151\n",
      "0.258651181715\n",
      "0.258649043166\n",
      "0.258646905507\n",
      "0.258644768741\n",
      "0.258642632869\n",
      "0.258640497895\n",
      "0.258638363819\n",
      "0.258636230644\n",
      "0.258634098372\n",
      "0.258631967004\n",
      "0.258629836542\n",
      "0.258627706987\n",
      "0.25862557834\n",
      "0.258623450603\n",
      "0.258621323777\n",
      "0.258619197864\n",
      "0.258617072863\n",
      "0.258614948777\n",
      "0.258612825606\n",
      "0.258610703352\n",
      "0.258608582014\n",
      "0.258606461595\n",
      "0.258604342096\n",
      "0.258602223516\n",
      "0.258600105858\n",
      "0.258597989121\n",
      "0.258595873307\n",
      "0.258593758417\n",
      "0.258591644451\n",
      "0.258589531411\n",
      "0.258587419297\n",
      "0.258585308109\n",
      "0.25858319785\n",
      "0.258581088519\n",
      "0.258578980117\n",
      "0.258576872646\n",
      "0.258574766105\n",
      "0.258572660497\n",
      "0.25857055582\n",
      "0.258568452077\n",
      "0.258566349269\n",
      "0.258564247395\n",
      "0.258562146456\n",
      "0.258560046454\n",
      "0.258557947389\n",
      "0.258555849262\n",
      "0.258553752073\n",
      "0.258551655824\n",
      "0.258549560515\n",
      "0.258547466147\n",
      "0.25854537272\n",
      "0.258543280236\n",
      "0.258541188694\n",
      "0.258539098097\n",
      "0.258537008444\n",
      "0.258534919736\n",
      "0.258532831974\n",
      "0.258530745159\n",
      "0.258528659291\n",
      "0.258526574371\n",
      "0.258524490401\n",
      "0.258522407379\n",
      "0.258520325308\n",
      "0.258518244188\n",
      "0.25851616402\n",
      "0.258514084803\n",
      "0.25851200654\n",
      "0.258509929231\n",
      "0.258507852876\n",
      "0.258505777476\n",
      "0.258503703032\n",
      "0.258501629545\n",
      "0.258499557014\n",
      "0.258497485442\n",
      "0.258495414828\n",
      "0.258493345173\n",
      "0.258491276478\n",
      "0.258489208744\n",
      "0.258487141971\n",
      "0.25848507616\n",
      "0.258483011311\n",
      "0.258480947426\n",
      "0.258478884505\n",
      "0.258476822548\n",
      "0.258474761556\n",
      "0.25847270153\n",
      "0.258470642471\n",
      "0.258468584379\n",
      "0.258466527255\n",
      "0.258464471099\n",
      "0.258462415912\n",
      "0.258460361695\n",
      "0.258458308448\n",
      "0.258456256172\n",
      "0.258454204868\n",
      "0.258452154537\n",
      "0.258450105178\n",
      "0.258448056792\n",
      "0.258446009381\n",
      "0.258443962944\n",
      "0.258441917483\n",
      "0.258439872998\n",
      "0.258437829489\n",
      "0.258435786958\n",
      "0.258433745405\n",
      "0.258431704829\n",
      "0.258429665233\n",
      "0.258427626617\n",
      "0.258425588981\n",
      "0.258423552325\n",
      "0.258421516651\n",
      "0.258419481959\n",
      "0.25841744825\n",
      "0.258415415523\n",
      "0.258413383781\n",
      "0.258411353023\n",
      "0.25840932325\n",
      "0.258407294462\n",
      "0.25840526666\n",
      "0.258403239845\n",
      "0.258401214018\n",
      "0.258399189178\n",
      "0.258397165326\n",
      "0.258395142464\n",
      "0.258393120591\n",
      "0.258391099707\n",
      "0.258389079815\n",
      "0.258387060914\n",
      "0.258385043004\n",
      "0.258383026087\n",
      "0.258381010162\n",
      "0.258378995231\n",
      "0.258376981293\n",
      "0.25837496835\n",
      "0.258372956402\n",
      "0.25837094545\n",
      "0.258368935493\n",
      "0.258366926534\n",
      "0.258364918571\n",
      "0.258362911606\n",
      "0.258360905639\n",
      "0.25835890067\n",
      "0.258356896701\n",
      "0.258354893731\n",
      "0.258352891762\n",
      "0.258350890794\n",
      "0.258348890826\n",
      "0.258346891861\n",
      "0.258344893897\n",
      "0.258342896937\n",
      "0.258340900979\n",
      "0.258338906026\n",
      "0.258336912076\n",
      "0.258334919131\n",
      "0.258332927192\n",
      "0.258330936258\n",
      "0.258328946331\n",
      "0.25832695741\n",
      "0.258324969496\n",
      "0.25832298259\n",
      "0.258320996692\n",
      "0.258319011803\n",
      "0.258317027922\n",
      "0.258315045051\n",
      "0.258313063191\n",
      "0.25831108234\n",
      "0.258309102501\n",
      "0.258307123673\n",
      "0.258305145857\n",
      "0.258303169053\n",
      "0.258301193262\n",
      "0.258299218484\n",
      "0.258297244719\n",
      "0.258295271969\n",
      "0.258293300234\n",
      "0.258291329513\n",
      "0.258289359808\n",
      "0.258287391118\n",
      "0.258285423445\n",
      "0.258283456789\n",
      "0.25828149115\n",
      "0.258279526528\n",
      "0.258277562924\n",
      "0.258275600339\n",
      "0.258273638773\n",
      "0.258271678226\n",
      "0.258269718698\n",
      "0.258267760191\n",
      "0.258265802704\n",
      "0.258263846238\n",
      "0.258261890794\n",
      "0.258259936371\n",
      "0.25825798297\n",
      "0.258256030592\n",
      "0.258254079237\n",
      "0.258252128905\n",
      "0.258250179597\n",
      "0.258248231313\n",
      "0.258246284054\n",
      "0.258244337819\n",
      "0.25824239261\n",
      "0.258240448426\n",
      "0.258238505269\n",
      "0.258236563137\n",
      "0.258234622033\n",
      "0.258232681956\n",
      "0.258230742906\n",
      "0.258228804884\n",
      "0.25822686789\n",
      "0.258224931926\n",
      "0.25822299699\n",
      "0.258221063083\n",
      "0.258219130206\n",
      "0.258217198359\n",
      "0.258215267543\n",
      "0.258213337757\n",
      "0.258211409003\n",
      "0.25820948128\n",
      "0.258207554588\n",
      "0.258205628929\n",
      "0.258203704302\n",
      "0.258201780709\n",
      "0.258199858148\n",
      "0.258197936621\n",
      "0.258196016127\n",
      "0.258194096668\n",
      "0.258192178243\n",
      "0.258190260853\n",
      "0.258188344498\n",
      "0.258186429179\n",
      "0.258184514895\n",
      "0.258182601648\n",
      "0.258180689436\n",
      "0.258178778262\n",
      "0.258176868124\n",
      "0.258174959024\n",
      "0.258173050961\n",
      "0.258171143936\n",
      "0.258169237949\n",
      "0.258167333001\n",
      "0.258165429092\n",
      "0.258163526222\n",
      "0.258161624391\n",
      "0.258159723599\n",
      "0.258157823848\n",
      "0.258155925137\n",
      "0.258154027466\n",
      "0.258152130836\n",
      "0.258150235248\n",
      "0.2581483407\n",
      "0.258146447194\n",
      "0.25814455473\n",
      "0.258142663308\n",
      "0.258140772929\n",
      "0.258138883592\n",
      "0.258136995298\n",
      "0.258135108048\n",
      "0.25813322184\n",
      "0.258131336677\n",
      "0.258129452557\n",
      "0.258127569482\n",
      "0.258125687451\n",
      "0.258123806465\n",
      "0.258121926523\n",
      "0.258120047627\n",
      "0.258118169776\n",
      "0.258116292971\n",
      "0.258114417212\n",
      "0.258112542499\n",
      "0.258110668832\n",
      "0.258108796212\n",
      "0.258106924639\n",
      "0.258105054113\n",
      "0.258103184634\n",
      "0.258101316202\n",
      "0.258099448818\n",
      "0.258097582482\n",
      "0.258095717194\n",
      "0.258093852955\n",
      "0.258091989764\n",
      "0.258090127622\n",
      "0.258088266528\n",
      "0.258086406484\n",
      "0.258084547489\n",
      "0.258082689544\n",
      "0.258080832648\n",
      "0.258078976803\n",
      "0.258077122007\n",
      "0.258075268262\n",
      "0.258073415567\n",
      "0.258071563923\n",
      "0.25806971333\n",
      "0.258067863788\n",
      "0.258066015298\n",
      "0.258064167858\n",
      "0.25806232147\n",
      "0.258060476134\n",
      "0.25805863185\n",
      "0.258056788618\n",
      "0.258054946438\n",
      "0.258053105311\n",
      "0.258051265236\n",
      "0.258049426214\n",
      "0.258047588245\n",
      "0.258045751329\n",
      "0.258043915466\n",
      "0.258042080657\n",
      "0.258040246901\n",
      "0.258038414199\n",
      "0.25803658255\n",
      "0.258034751956\n",
      "0.258032922415\n",
      "0.258031093929\n",
      "0.258029266497\n",
      "0.25802744012\n",
      "0.258025614797\n",
      "0.258023790529\n",
      "0.258021967316\n",
      "0.258020145158\n",
      "0.258018324056\n",
      "0.258016504008\n",
      "0.258014685016\n",
      "0.258012867079\n",
      "0.258011050198\n",
      "0.258009234373\n",
      "0.258007419604\n",
      "0.25800560589\n",
      "0.258003793233\n",
      "0.258001981632\n",
      "0.258000171087\n",
      "0.257998361598\n",
      "0.257996553166\n",
      "0.257994745791\n",
      "0.257992939472\n",
      "0.25799113421\n",
      "0.257989330005\n",
      "0.257987526857\n",
      "0.257985724765\n",
      "0.257983923732\n",
      "0.257982123755\n",
      "0.257980324835\n",
      "0.257978526973\n",
      "0.257976730169\n",
      "0.257974934421\n",
      "0.257973139732\n",
      "0.2579713461\n",
      "0.257969553526\n",
      "0.25796776201\n",
      "0.257965971552\n",
      "0.257964182151\n",
      "0.257962393809\n",
      "0.257960606524\n",
      "0.257958820298\n",
      "0.25795703513\n",
      "0.25795525102\n",
      "0.257953467969\n",
      "0.257951685976\n",
      "0.257949905041\n",
      "0.257948125165\n",
      "0.257946346347\n",
      "0.257944568588\n",
      "0.257942791887\n",
      "0.257941016245\n",
      "0.257939241662\n",
      "0.257937468137\n",
      "0.257935695671\n",
      "0.257933924263\n",
      "0.257932153915\n",
      "0.257930384625\n",
      "0.257928616394\n",
      "0.257926849222\n",
      "0.257925083109\n",
      "0.257923318054\n",
      "0.257921554059\n",
      "0.257919791122\n",
      "0.257918029245\n",
      "0.257916268426\n",
      "0.257914508666\n",
      "0.257912749966\n",
      "0.257910992324\n",
      "0.257909235741\n",
      "0.257907480217\n",
      "0.257905725753\n",
      "0.257903972347\n",
      "0.25790222\n",
      "0.257900468712\n",
      "0.257898718483\n",
      "0.257896969313\n",
      "0.257895221202\n",
      "0.25789347415\n",
      "0.257891728157\n",
      "0.257889983222\n",
      "0.257888239347\n",
      "0.25788649653\n",
      "0.257884754773\n",
      "0.257883014074\n",
      "0.257881274434\n",
      "0.257879535852\n",
      "0.25787779833\n",
      "0.257876061866\n",
      "0.25787432646\n",
      "0.257872592114\n",
      "0.257870858826\n",
      "0.257869126596\n",
      "0.257867395425\n",
      "0.257865665313\n",
      "0.257863936259\n",
      "0.257862208263\n",
      "0.257860481326\n",
      "0.257858755447\n",
      "0.257857030626\n",
      "0.257855306863\n",
      "0.257853584159\n",
      "0.257851862512\n",
      "0.257850141924\n",
      "0.257848422393\n",
      "0.257846703921\n",
      "0.257844986506\n",
      "0.257843270149\n",
      "0.25784155485\n",
      "0.257839840608\n",
      "0.257838127424\n",
      "0.257836415297\n",
      "0.257834704228\n",
      "0.257832994216\n",
      "0.257831285261\n",
      "0.257829577364\n",
      "0.257827870524\n",
      "0.25782616474\n",
      "0.257824460014\n",
      "0.257822756344\n",
      "0.257821053731\n",
      "0.257819352175\n",
      "0.257817651675\n",
      "0.257815952232\n",
      "0.257814253846\n",
      "0.257812556515\n",
      "0.257810860241\n",
      "0.257809165022\n",
      "0.25780747086\n",
      "0.257805777754\n",
      "0.257804085703\n",
      "0.257802394708\n",
      "0.257800704769\n",
      "0.257799015885\n",
      "0.257797328056\n",
      "0.257795641283\n",
      "0.257793955564\n",
      "0.257792270901\n",
      "0.257790587293\n",
      "0.257788904739\n",
      "0.25778722324\n",
      "0.257785542795\n",
      "0.257783863405\n",
      "0.257782185069\n",
      "0.257780507787\n",
      "0.257778831559\n",
      "0.257777156385\n",
      "0.257775482265\n",
      "0.257773809198\n",
      "0.257772137184\n",
      "0.257770466224\n",
      "0.257768796318\n",
      "0.257767127464\n",
      "0.257765459663\n",
      "0.257763792914\n",
      "0.257762127219\n",
      "0.257760462575\n",
      "0.257758798984\n",
      "0.257757136446\n",
      "0.257755474959\n",
      "0.257753814524\n",
      "0.25775215514\n",
      "0.257750496808\n",
      "0.257748839528\n",
      "0.257747183298\n",
      "0.25774552812\n",
      "0.257743873992\n",
      "0.257742220915\n",
      "0.257740568889\n",
      "0.257738917913\n",
      "0.257737267987\n",
      "0.257735619111\n",
      "0.257733971285\n",
      "0.257732324509\n",
      "0.257730678782\n",
      "0.257729034104\n",
      "0.257727390476\n",
      "0.257725747896\n",
      "0.257724106366\n",
      "0.257722465883\n",
      "0.257720826449\n",
      "0.257719188064\n",
      "0.257717550726\n",
      "0.257715914436\n",
      "0.257714279193\n",
      "0.257712644999\n",
      "0.257711011851\n",
      "0.25770937975\n",
      "0.257707748696\n",
      "0.257706118689\n",
      "0.257704489728\n",
      "0.257702861813\n",
      "0.257701234945\n",
      "0.257699609122\n",
      "0.257697984344\n",
      "0.257696360612\n",
      "0.257694737926\n",
      "0.257693116284\n",
      "0.257691495687\n",
      "0.257689876134\n",
      "0.257688257626\n",
      "0.257686640162\n",
      "0.257685023741\n",
      "0.257683408365\n",
      "0.257681794031\n",
      "0.257680180741\n",
      "0.257678568494\n",
      "0.25767695729\n",
      "0.257675347128\n",
      "0.257673738008\n",
      "0.25767212993\n",
      "0.257670522895\n",
      "0.2576689169\n",
      "0.257667311947\n",
      "0.257665708036\n",
      "0.257664105165\n",
      "0.257662503334\n",
      "0.257660902544\n",
      "0.257659302794\n",
      "0.257657704084\n",
      "0.257656106414\n",
      "0.257654509783\n",
      "0.257652914191\n",
      "0.257651319638\n",
      "0.257649726124\n",
      "0.257648133648\n",
      "0.25764654221\n",
      "0.25764495181\n",
      "0.257643362448\n",
      "0.257641774123\n",
      "0.257640186835\n",
      "0.257638600584\n",
      "0.25763701537\n",
      "0.257635431192\n",
      "0.257633848049\n",
      "0.257632265943\n",
      "0.257630684872\n",
      "0.257629104836\n",
      "0.257627525836\n",
      "0.257625947869\n",
      "0.257624370938\n",
      "0.25762279504\n",
      "0.257621220176\n",
      "0.257619646346\n",
      "0.257618073549\n",
      "0.257616501786\n",
      "0.257614931055\n",
      "0.257613361356\n",
      "0.257611792689\n",
      "0.257610225055\n",
      "0.257608658452\n",
      "0.25760709288\n",
      "0.257605528339\n",
      "0.257603964829\n",
      "0.257602402349\n",
      "0.2576008409\n",
      "0.25759928048\n",
      "0.25759772109\n",
      "0.257596162729\n",
      "0.257594605397\n",
      "0.257593049093\n",
      "0.257591493818\n",
      "0.257589939571\n",
      "0.257588386352\n",
      "0.25758683416\n",
      "0.257585282995\n",
      "0.257583732857\n",
      "0.257582183745\n",
      "0.25758063566\n",
      "0.2575790886\n",
      "0.257577542566\n",
      "0.257575997557\n",
      "0.257574453574\n",
      "0.257572910614\n",
      "0.257571368679\n",
      "0.257569827768\n",
      "0.257568287881\n",
      "0.257566749017\n",
      "0.257565211175\n",
      "0.257563674357\n",
      "0.257562138561\n",
      "0.257560603787\n",
      "0.257559070034\n",
      "0.257557537303\n",
      "0.257556005593\n",
      "0.257554474904\n",
      "0.257552945235\n",
      "0.257551416585\n",
      "0.257549888956\n",
      "0.257548362346\n",
      "0.257546836755\n",
      "0.257545312183\n",
      "0.257543788629\n",
      "0.257542266093\n",
      "0.257540744574\n",
      "0.257539224073\n",
      "0.257537704589\n",
      "0.257536186121\n",
      "0.25753466867\n",
      "0.257533152234\n",
      "0.257531636815\n",
      "0.25753012241\n",
      "0.25752860902\n",
      "0.257527096645\n",
      "0.257525585284\n",
      "0.257524074936\n",
      "0.257522565602\n",
      "0.257521057281\n",
      "0.257519549973\n",
      "0.257518043677\n",
      "0.257516538393\n",
      "0.257515034121\n",
      "0.25751353086\n",
      "0.25751202861\n",
      "0.25751052737\n",
      "0.257509027141\n",
      "0.257507527921\n",
      "0.257506029711\n",
      "0.25750453251\n",
      "0.257503036318\n",
      "0.257501541133\n",
      "0.257500046957\n",
      "0.257498553788\n",
      "0.257497061627\n",
      "0.257495570472\n",
      "0.257494080324\n",
      "0.257492591182\n",
      "0.257491103045\n",
      "0.257489615914\n",
      "0.257488129788\n",
      "0.257486644666\n",
      "0.257485160548\n",
      "0.257483677434\n",
      "0.257482195324\n",
      "0.257480714216\n",
      "0.257479234111\n",
      "0.257477755008\n",
      "0.257476276907\n",
      "0.257474799808\n",
      "0.257473323709\n",
      "0.257471848611\n",
      "0.257470374513\n",
      "0.257468901415\n",
      "0.257467429317\n",
      "0.257465958217\n",
      "0.257464488117\n",
      "0.257463019014\n",
      "0.257461550909\n",
      "0.257460083802\n",
      "0.257458617692\n",
      "0.257457152578\n",
      "0.257455688461\n",
      "0.257454225339\n",
      "0.257452763213\n",
      "0.257451302082\n",
      "0.257449841945\n",
      "0.257448382803\n",
      "0.257446924654\n",
      "0.257445467499\n",
      "0.257444011337\n",
      "0.257442556167\n",
      "0.257441101989\n",
      "0.257439648803\n",
      "0.257438196608\n",
      "0.257436745405\n",
      "0.257435295191\n",
      "0.257433845968\n",
      "0.257432397734\n",
      "0.25743095049\n",
      "0.257429504234\n",
      "0.257428058966\n",
      "0.257426614687\n",
      "0.257425171395\n",
      "0.25742372909\n",
      "0.257422287771\n",
      "0.257420847439\n",
      "0.257419408093\n",
      "0.257417969732\n",
      "0.257416532356\n",
      "0.257415095964\n",
      "0.257413660556\n",
      "0.257412226132\n",
      "0.257410792691\n",
      "0.257409360233\n",
      "0.257407928758\n",
      "0.257406498264\n",
      "0.257405068751\n",
      "0.25740364022\n",
      "0.257402212669\n",
      "0.257400786098\n",
      "0.257399360507\n",
      "0.257397935895\n",
      "0.257396512262\n",
      "0.257395089608\n",
      "0.257393667931\n",
      "0.257392247231\n",
      "0.257390827509\n",
      "0.257389408763\n",
      "0.257387990993\n",
      "0.257386574199\n",
      "0.257385158381\n",
      "0.257383743536\n",
      "0.257382329667\n",
      "0.257380916771\n",
      "0.257379504848\n",
      "0.257378093899\n",
      "0.257376683922\n",
      "0.257375274917\n",
      "0.257373866883\n",
      "0.257372459821\n",
      "0.257371053729\n",
      "0.257369648608\n",
      "0.257368244456\n",
      "0.257366841273\n",
      "0.25736543906\n",
      "0.257364037815\n",
      "0.257362637537\n",
      "0.257361238227\n",
      "0.257359839884\n",
      "0.257358442508\n",
      "0.257357046097\n",
      "0.257355650652\n",
      "0.257354256173\n",
      "0.257352862658\n",
      "0.257351470107\n",
      "0.257350078519\n",
      "0.257348687895\n",
      "0.257347298234\n",
      "0.257345909535\n",
      "0.257344521798\n",
      "0.257343135022\n",
      "0.257341749207\n",
      "0.257340364352\n",
      "0.257338980457\n",
      "0.257337597522\n",
      "0.257336215545\n",
      "0.257334834527\n",
      "0.257333454467\n",
      "0.257332075365\n",
      "0.257330697219\n",
      "0.25732932003\n",
      "0.257327943798\n",
      "0.25732656852\n",
      "0.257325194198\n",
      "0.257323820831\n",
      "0.257322448417\n",
      "0.257321076958\n",
      "0.257319706451\n",
      "0.257318336897\n",
      "0.257316968295\n",
      "0.257315600645\n",
      "0.257314233946\n",
      "0.257312868198\n",
      "0.2573115034\n",
      "0.257310139552\n",
      "0.257308776653\n",
      "0.257307414703\n",
      "0.257306053701\n",
      "0.257304693647\n",
      "0.25730333454\n",
      "0.25730197638\n",
      "0.257300619166\n",
      "0.257299262898\n",
      "0.257297907575\n",
      "0.257296553197\n",
      "0.257295199763\n",
      "0.257293847273\n",
      "0.257292495727\n",
      "0.257291145123\n",
      "0.257289795461\n",
      "0.257288446741\n",
      "0.257287098963\n",
      "0.257285752125\n",
      "0.257284406228\n",
      "0.25728306127\n",
      "0.257281717252\n",
      "0.257280374172\n",
      "0.25727903203\n",
      "0.257277690827\n",
      "0.25727635056\n",
      "0.257275011231\n",
      "0.257273672837\n",
      "0.257272335379\n",
      "0.257270998857\n",
      "0.257269663269\n",
      "0.257268328615\n",
      "0.257266994895\n",
      "0.257265662109\n",
      "0.257264330254\n",
      "0.257262999332\n",
      "0.257261669342\n",
      "0.257260340283\n",
      "0.257259012154\n",
      "0.257257684955\n",
      "0.257256358686\n",
      "0.257255033346\n",
      "0.257253708935\n",
      "0.257252385451\n",
      "0.257251062895\n",
      "0.257249741266\n",
      "0.257248420563\n",
      "0.257247100787\n",
      "0.257245781936\n",
      "0.257244464009\n",
      "0.257243147007\n",
      "0.257241830929\n",
      "0.257240515774\n",
      "0.257239201542\n",
      "0.257237888233\n",
      "0.257236575845\n",
      "0.257235264378\n",
      "0.257233953832\n",
      "0.257232644206\n",
      "0.2572313355\n",
      "0.257230027713\n",
      "0.257228720844\n",
      "0.257227414894\n",
      "0.257226109861\n",
      "0.257224805745\n",
      "0.257223502546\n",
      "0.257222200263\n",
      "0.257220898895\n",
      "0.257219598442\n",
      "0.257218298903\n",
      "0.257217000278\n",
      "0.257215702567\n",
      "0.257214405768\n",
      "0.257213109881\n",
      "0.257211814907\n",
      "0.257210520843\n",
      "0.25720922769\n",
      "0.257207935447\n",
      "0.257206644114\n",
      "0.257205353689\n",
      "0.257204064173\n",
      "0.257202775565\n",
      "0.257201487865\n",
      "0.257200201071\n",
      "0.257198915184\n",
      "0.257197630203\n",
      "0.257196346126\n",
      "0.257195062955\n",
      "0.257193780688\n",
      "0.257192499324\n",
      "0.257191218863\n",
      "0.257189939305\n",
      "0.257188660649\n",
      "0.257187382895\n",
      "0.257186106041\n",
      "0.257184830088\n",
      "0.257183555035\n",
      "0.257182280881\n",
      "0.257181007626\n",
      "0.257179735269\n",
      "0.257178463809\n",
      "0.257177193247\n",
      "0.257175923581\n",
      "0.257174654812\n",
      "0.257173386938\n",
      "0.257172119959\n",
      "0.257170853874\n",
      "0.257169588683\n",
      "0.257168324385\n",
      "0.25716706098\n",
      "0.257165798468\n",
      "0.257164536847\n",
      "0.257163276117\n",
      "0.257162016277\n",
      "0.257160757328\n",
      "0.257159499268\n",
      "0.257158242097\n",
      "0.257156985814\n",
      "0.257155730419\n",
      "0.257154475912\n",
      "0.257153222291\n",
      "0.257151969556\n",
      "0.257150717707\n",
      "0.257149466743\n",
      "0.257148216663\n",
      "0.257146967467\n",
      "0.257145719155\n",
      "0.257144471725\n",
      "0.257143225178\n",
      "0.257141979512\n",
      "0.257140734728\n",
      "0.257139490824\n",
      "0.2571382478\n",
      "0.257137005656\n",
      "0.25713576439\n",
      "0.257134524003\n",
      "0.257133284494\n",
      "0.257132045862\n",
      "0.257130808107\n",
      "0.257129571228\n",
      "0.257128335224\n",
      "0.257127100095\n",
      "0.257125865841\n",
      "0.257124632461\n",
      "0.257123399954\n",
      "0.25712216832\n",
      "0.257120937558\n",
      "0.257119707668\n",
      "0.257118478649\n",
      "0.2571172505\n",
      "0.257116023221\n",
      "0.257114796812\n",
      "0.257113571271\n",
      "0.257112346599\n",
      "0.257111122795\n",
      "0.257109899858\n",
      "0.257108677787\n",
      "0.257107456582\n",
      "0.257106236243\n",
      "0.257105016769\n",
      "0.257103798159\n",
      "0.257102580412\n",
      "0.257101363529\n",
      "0.257100147509\n",
      "0.257098932351\n",
      "0.257097718054\n",
      "0.257096504618\n",
      "0.257095292043\n",
      "0.257094080327\n",
      "0.25709286947\n",
      "0.257091659473\n",
      "0.257090450333\n",
      "0.257089242051\n",
      "0.257088034626\n",
      "0.257086828057\n",
      "0.257085622344\n",
      "0.257084417486\n",
      "0.257083213483\n",
      "0.257082010334\n",
      "0.257080808039\n",
      "0.257079606597\n",
      "0.257078406007\n",
      "0.257077206269\n",
      "0.257076007382\n",
      "0.257074809346\n",
      "0.25707361216\n",
      "0.257072415824\n",
      "0.257071220336\n",
      "0.257070025698\n",
      "0.257068831906\n",
      "0.257067638963\n",
      "0.257066446866\n",
      "0.257065255615\n",
      "0.257064065209\n",
      "0.257062875649\n",
      "0.257061686933\n",
      "0.257060499061\n",
      "0.257059312032\n",
      "0.257058125846\n",
      "0.257056940502\n",
      "0.257055756\n",
      "0.257054572338\n",
      "0.257053389517\n",
      "0.257052207536\n",
      "0.257051026394\n",
      "0.257049846091\n",
      "0.257048666626\n",
      "0.257047487998\n",
      "0.257046310208\n",
      "0.257045133253\n",
      "0.257043957135\n",
      "0.257042781852\n",
      "0.257041607403\n",
      "0.257040433789\n",
      "0.257039261008\n",
      "0.25703808906\n",
      "0.257036917945\n",
      "0.257035747661\n",
      "0.257034578208\n",
      "0.257033409587\n",
      "0.257032241795\n",
      "0.257031074832\n",
      "0.257029908699\n",
      "0.257028743394\n",
      "0.257027578916\n",
      "0.257026415266\n",
      "0.257025252443\n",
      "0.257024090445\n",
      "0.257022929273\n",
      "0.257021768926\n",
      "0.257020609403\n",
      "0.257019450704\n",
      "0.257018292828\n",
      "0.257017135774\n",
      "0.257015979543\n",
      "0.257014824133\n",
      "0.257013669543\n",
      "0.257012515774\n",
      "0.257011362825\n",
      "0.257010210695\n",
      "0.257009059383\n",
      "0.257007908889\n",
      "0.257006759213\n",
      "0.257005610353\n",
      "0.25700446231\n",
      "0.257003315082\n",
      "0.257002168669\n",
      "0.257001023071\n",
      "0.256999878286\n",
      "0.256998734315\n",
      "0.256997591157\n",
      "0.256996448811\n",
      "0.256995307276\n",
      "0.256994166552\n",
      "0.256993026639\n",
      "0.256991887535\n",
      "0.256990749241\n",
      "0.256989611756\n",
      "0.256988475078\n",
      "0.256987339208\n",
      "0.256986204145\n",
      "0.256985069889\n",
      "0.256983936438\n",
      "0.256982803792\n",
      "0.256981671951\n",
      "0.256980540914\n",
      "0.256979410681\n",
      "0.25697828125\n",
      "0.256977152622\n",
      "0.256976024795\n",
      "0.25697489777\n",
      "0.256973771545\n",
      "0.25697264612\n",
      "0.256971521494\n",
      "0.256970397667\n",
      "0.256969274639\n",
      "0.256968152408\n",
      "0.256967030974\n",
      "0.256965910337\n",
      "0.256964790495\n",
      "0.256963671449\n",
      "0.256962553198\n",
      "0.256961435741\n",
      "0.256960319077\n",
      "0.256959203206\n",
      "0.256958088128\n",
      "0.256956973842\n",
      "0.256955860347\n",
      "0.256954747643\n",
      "0.256953635729\n",
      "0.256952524604\n",
      "0.256951414268\n",
      "0.256950304721\n",
      "0.256949195961\n",
      "0.256948087989\n",
      "0.256946980803\n",
      "0.256945874403\n",
      "0.256944768789\n",
      "0.25694366396\n",
      "0.256942559915\n",
      "0.256941456653\n",
      "0.256940354175\n",
      "0.25693925248\n",
      "0.256938151566\n",
      "0.256937051434\n",
      "0.256935952082\n",
      "0.256934853511\n",
      "0.25693375572\n",
      "0.256932658707\n",
      "0.256931562473\n",
      "0.256930467017\n",
      "0.256929372339\n",
      "0.256928278437\n",
      "0.256927185311\n",
      "0.256926092961\n",
      "0.256925001386\n",
      "0.256923910585\n",
      "0.256922820559\n",
      "0.256921731305\n",
      "0.256920642824\n",
      "0.256919555116\n",
      "0.256918468179\n",
      "0.256917382012\n",
      "0.256916296617\n",
      "0.256915211991\n",
      "0.256914128134\n",
      "0.256913045046\n",
      "0.256911962726\n",
      "0.256910881173\n",
      "0.256909800388\n",
      "0.256908720368\n",
      "0.256907641115\n",
      "0.256906562627\n",
      "0.256905484903\n",
      "0.256904407943\n",
      "0.256903331747\n",
      "0.256902256313\n",
      "0.256901181642\n",
      "0.256900107733\n",
      "0.256899034584\n",
      "0.256897962197\n",
      "0.256896890569\n",
      "0.2568958197\n",
      "0.256894749591\n",
      "0.256893680239\n",
      "0.256892611646\n",
      "0.256891543809\n",
      "0.256890476729\n",
      "0.256889410405\n",
      "0.256888344836\n",
      "0.256887280022\n",
      "0.256886215962\n",
      "0.256885152655\n",
      "0.256884090102\n",
      "0.256883028301\n",
      "0.256881967252\n",
      "0.256880906955\n",
      "0.256879847408\n",
      "0.256878788611\n",
      "0.256877730564\n",
      "0.256876673266\n",
      "0.256875616716\n",
      "0.256874560914\n",
      "0.256873505859\n",
      "0.256872451552\n",
      "0.25687139799\n",
      "0.256870345173\n",
      "0.256869293102\n",
      "0.256868241775\n",
      "0.256867191192\n",
      "0.256866141352\n",
      "0.256865092255\n",
      "0.2568640439\n",
      "0.256862996286\n",
      "0.256861949414\n",
      "0.256860903282\n",
      "0.256859857889\n",
      "0.256858813236\n",
      "0.256857769321\n",
      "0.256856726145\n",
      "0.256855683706\n",
      "0.256854642004\n",
      "0.256853601038\n",
      "0.256852560808\n",
      "0.256851521313\n",
      "0.256850482553\n",
      "0.256849444527\n",
      "0.256848407234\n",
      "0.256847370675\n",
      "0.256846334847\n",
      "0.256845299752\n",
      "0.256844265387\n",
      "0.256843231754\n",
      "0.25684219885\n",
      "0.256841166676\n",
      "0.25684013523\n",
      "0.256839104513\n",
      "0.256838074524\n",
      "0.256837045262\n",
      "0.256836016727\n",
      "0.256834988917\n",
      "0.256833961834\n",
      "0.256832935475\n",
      "0.25683190984\n",
      "0.256830884929\n",
      "0.256829860741\n",
      "0.256828837276\n",
      "0.256827814534\n",
      "0.256826792512\n",
      "0.256825771212\n",
      "0.256824750631\n",
      "0.256823730771\n",
      "0.25682271163\n",
      "0.256821693207\n",
      "0.256820675503\n",
      "0.256819658516\n",
      "0.256818642246\n",
      "0.256817626692\n",
      "0.256816611854\n",
      "0.256815597732\n",
      "0.256814584324\n",
      "0.25681357163\n",
      "0.25681255965\n",
      "0.256811548382\n",
      "0.256810537828\n",
      "0.256809527985\n",
      "0.256808518853\n",
      "0.256807510432\n",
      "0.256806502721\n",
      "0.256805495719\n",
      "0.256804489427\n",
      "0.256803483843\n",
      "0.256802478967\n",
      "0.256801474798\n",
      "0.256800471336\n",
      "0.256799468581\n",
      "0.25679846653\n",
      "0.256797465185\n",
      "0.256796464545\n",
      "0.256795464608\n",
      "0.256794465375\n",
      "0.256793466844\n",
      "0.256792469016\n",
      "0.256791471889\n",
      "0.256790475464\n",
      "0.256789479739\n",
      "0.256788484714\n",
      "0.256787490388\n",
      "0.256786496762\n",
      "0.256785503833\n",
      "0.256784511603\n",
      "0.256783520069\n",
      "0.256782529232\n",
      "0.256781539092\n",
      "0.256780549646\n",
      "0.256779560896\n",
      "0.25677857284\n",
      "0.256777585478\n",
      "0.256776598809\n",
      "0.256775612833\n",
      "0.256774627549\n",
      "0.256773642957\n",
      "0.256772659055\n",
      "0.256771675845\n",
      "0.256770693324\n",
      "0.256769711492\n",
      "0.256768730349\n",
      "0.256767749894\n",
      "0.256766770128\n",
      "0.256765791048\n",
      "0.256764812654\n",
      "0.256763834947\n",
      "0.256762857925\n",
      "0.256761881589\n",
      "0.256760905936\n",
      "0.256759930967\n",
      "0.256758956682\n",
      "0.256757983079\n",
      "0.256757010158\n",
      "0.256756037919\n",
      "0.256755066361\n",
      "0.256754095483\n",
      "0.256753125286\n",
      "0.256752155767\n",
      "0.256751186928\n",
      "0.256750218766\n",
      "0.256749251282\n",
      "0.256748284476\n",
      "0.256747318346\n",
      "0.256746352892\n",
      "0.256745388114\n",
      "0.25674442401\n",
      "0.256743460581\n",
      "0.256742497826\n",
      "0.256741535744\n",
      "0.256740574335\n",
      "0.256739613598\n",
      "0.256738653532\n",
      "0.256737694138\n",
      "0.256736735414\n",
      "0.25673577736\n",
      "0.256734819976\n",
      "0.25673386326\n",
      "0.256732907213\n",
      "0.256731951834\n",
      "0.256730997121\n",
      "0.256730043076\n",
      "0.256729089697\n",
      "0.256728136983\n",
      "0.256727184934\n",
      "0.25672623355\n",
      "0.25672528283\n",
      "0.256724332773\n",
      "0.256723383379\n",
      "0.256722434648\n",
      "0.256721486578\n",
      "0.256720539169\n",
      "0.256719592421\n",
      "0.256718646334\n",
      "0.256717700906\n",
      "0.256716756137\n",
      "0.256715812026\n",
      "0.256714868573\n",
      "0.256713925778\n",
      "0.25671298364\n",
      "0.256712042158\n",
      "0.256711101332\n",
      "0.256710161161\n",
      "0.256709221645\n",
      "0.256708282783\n",
      "0.256707344575\n",
      "0.25670640702\n",
      "0.256705470117\n",
      "0.256704533866\n",
      "0.256703598267\n",
      "0.256702663319\n",
      "0.256701729021\n",
      "0.256700795373\n",
      "0.256699862375\n",
      "0.256698930025\n",
      "0.256697998323\n",
      "0.256697067269\n",
      "0.256696136863\n",
      "0.256695207103\n",
      "0.256694277989\n",
      "0.25669334952\n",
      "0.256692421697\n",
      "0.256691494518\n",
      "0.256690567983\n",
      "0.256689642091\n",
      "0.256688716843\n",
      "0.256687792237\n",
      "0.256686868272\n",
      "0.256685944949\n",
      "0.256685022267\n",
      "0.256684100224\n",
      "0.256683178822\n",
      "0.256682258058\n",
      "0.256681337934\n",
      "0.256680418447\n",
      "0.256679499598\n",
      "0.256678581386\n",
      "0.25667766381\n",
      "0.256676746871\n",
      "0.256675830566\n",
      "0.256674914897\n",
      "0.256673999862\n",
      "0.256673085461\n",
      "0.256672171693\n",
      "0.256671258558\n",
      "0.256670346056\n",
      "0.256669434185\n",
      "0.256668522945\n",
      "0.256667612336\n",
      "0.256666702357\n",
      "0.256665793007\n",
      "0.256664884287\n",
      "0.256663976195\n",
      "0.256663068732\n",
      "0.256662161895\n",
      "0.256661255686\n",
      "0.256660350104\n",
      "0.256659445147\n",
      "0.256658540815\n",
      "0.256657637109\n",
      "0.256656734027\n",
      "0.256655831569\n",
      "0.256654929734\n",
      "0.256654028521\n",
      "0.256653127932\n",
      "0.256652227963\n",
      "0.256651328616\n",
      "0.25665042989\n",
      "0.256649531784\n",
      "0.256648634298\n",
      "0.256647737431\n",
      "0.256646841182\n",
      "0.256645945552\n",
      "0.256645050539\n",
      "0.256644156143\n",
      "0.256643262363\n",
      "0.2566423692\n",
      "0.256641476652\n",
      "0.25664058472\n",
      "0.256639693401\n",
      "0.256638802697\n",
      "0.256637912606\n",
      "0.256637023128\n",
      "0.256636134263\n",
      "0.256635246009\n",
      "0.256634358367\n",
      "0.256633471335\n",
      "0.256632584914\n",
      "0.256631699103\n",
      "0.256630813901\n",
      "0.256629929308\n",
      "0.256629045323\n",
      "0.256628161946\n",
      "0.256627279176\n",
      "0.256626397013\n",
      "0.256625515456\n",
      "0.256624634505\n",
      "0.256623754159\n",
      "0.256622874418\n",
      "0.256621995281\n",
      "0.256621116748\n",
      "0.256620238818\n",
      "0.25661936149\n",
      "0.256618484765\n",
      "0.256617608641\n",
      "0.256616733119\n",
      "0.256615858197\n",
      "0.256614983875\n",
      "0.256614110153\n",
      "0.25661323703\n",
      "0.256612364505\n",
      "0.256611492579\n",
      "0.25661062125\n",
      "0.256609750518\n",
      "0.256608880383\n",
      "0.256608010844\n",
      "0.256607141901\n",
      "0.256606273552\n",
      "0.256605405798\n",
      "0.256604538638\n",
      "0.256603672072\n",
      "0.256602806099\n",
      "0.256601940718\n",
      "0.256601075929\n",
      "0.256600211732\n",
      "0.256599348126\n",
      "0.25659848511\n",
      "0.256597622685\n",
      "0.256596760849\n",
      "0.256595899602\n",
      "0.256595038943\n",
      "0.256594178873\n",
      "0.25659331939\n",
      "0.256592460494\n",
      "0.256591602185\n",
      "0.256590744462\n",
      "0.256589887324\n",
      "0.256589030771\n",
      "0.256588174803\n",
      "0.256587319419\n",
      "0.256586464618\n",
      "0.256585610401\n",
      "0.256584756766\n",
      "0.256583903713\n",
      "0.256583051242\n",
      "0.256582199352\n",
      "0.256581348042\n",
      "0.256580497312\n",
      "0.256579647162\n",
      "0.256578797592\n",
      "0.256577948599\n",
      "0.256577100185\n",
      "0.256576252349\n",
      "0.256575405089\n",
      "0.256574558406\n",
      "0.2565737123\n",
      "0.256572866769\n",
      "0.256572021813\n",
      "0.256571177432\n",
      "0.256570333625\n",
      "0.256569490392\n",
      "0.256568647732\n",
      "0.256567805644\n",
      "0.256566964129\n",
      "0.256566123186\n",
      "0.256565282814\n",
      "0.256564443013\n",
      "0.256563603782\n",
      "0.256562765121\n",
      "0.256561927029\n",
      "0.256561089506\n",
      "0.256560252551\n",
      "0.256559416164\n",
      "0.256558580345\n",
      "0.256557745092\n",
      "0.256556910406\n",
      "0.256556076286\n",
      "0.256555242731\n",
      "0.256554409741\n",
      "0.256553577315\n",
      "0.256552745454\n",
      "0.256551914156\n",
      "0.256551083421\n",
      "0.256550253249\n",
      "0.256549423639\n",
      "0.25654859459\n",
      "0.256547766103\n",
      "0.256546938176\n",
      "0.256546110809\n",
      "0.256545284002\n",
      "0.256544457754\n",
      "0.256543632065\n",
      "0.256542806934\n",
      "0.256541982361\n",
      "0.256541158345\n",
      "0.256540334886\n",
      "0.256539511983\n",
      "0.256538689636\n",
      "0.256537867844\n",
      "0.256537046608\n",
      "0.256536225926\n",
      "0.256535405797\n",
      "0.256534586223\n",
      "0.256533767201\n",
      "0.256532948732\n",
      "0.256532130815\n",
      "0.256531313449\n",
      "0.256530496635\n",
      "0.256529680371\n",
      "0.256528864657\n",
      "0.256528049494\n",
      "0.256527234879\n",
      "0.256526420813\n",
      "0.256525607296\n",
      "0.256524794326\n",
      "0.256523981904\n",
      "0.256523170028\n",
      "0.256522358699\n",
      "0.256521547916\n",
      "0.256520737678\n",
      "0.256519927986\n",
      "0.256519118838\n",
      "0.256518310234\n",
      "0.256517502174\n",
      "0.256516694657\n",
      "0.256515887682\n",
      "0.25651508125\n",
      "0.256514275359\n",
      "0.25651347001\n",
      "0.256512665202\n",
      "0.256511860934\n",
      "0.256511057206\n",
      "0.256510254018\n",
      "0.256509451368\n",
      "0.256508649257\n",
      "0.256507847684\n",
      "0.256507046649\n",
      "0.25650624615\n",
      "0.256505446189\n",
      "0.256504646763\n",
      "0.256503847874\n",
      "0.25650304952\n",
      "0.2565022517\n",
      "0.256501454415\n",
      "0.256500657664\n",
      "0.256499861446\n",
      "0.256499065762\n",
      "0.25649827061\n",
      "0.25649747599\n",
      "0.256496681902\n",
      "0.256495888344\n",
      "0.256495095318\n",
      "0.256494302822\n",
      "0.256493510856\n",
      "0.256492719419\n",
      "0.256491928511\n",
      "0.256491138131\n",
      "0.25649034828\n",
      "0.256489558956\n",
      "0.256488770159\n",
      "0.256487981889\n",
      "0.256487194145\n",
      "0.256486406927\n",
      "0.256485620234\n",
      "0.256484834066\n",
      "0.256484048422\n",
      "0.256483263302\n",
      "0.256482478706\n",
      "0.256481694633\n",
      "0.256480911082\n",
      "0.256480128054\n",
      "0.256479345547\n",
      "0.256478563562\n",
      "0.256477782097\n",
      "0.256477001153\n",
      "0.256476220729\n",
      "0.256475440824\n",
      "0.256474661438\n",
      "0.256473882571\n",
      "0.256473104222\n",
      "0.256472326391\n",
      "0.256471549076\n",
      "0.256470772279\n",
      "0.256469995998\n",
      "0.256469220233\n",
      "0.256468444984\n",
      "0.256467670249\n",
      "0.256466896029\n",
      "0.256466122324\n",
      "0.256465349132\n",
      "0.256464576453\n",
      "0.256463804287\n",
      "0.256463032633\n",
      "0.256462261492\n",
      "0.256461490862\n",
      "0.256460720743\n",
      "0.256459951135\n",
      "0.256459182037\n",
      "0.256458413448\n",
      "0.256457645369\n",
      "0.256456877799\n",
      "0.256456110738\n",
      "0.256455344184\n",
      "0.256454578138\n",
      "0.256453812599\n",
      "0.256453047567\n",
      "0.256452283041\n",
      "0.256451519021\n",
      "0.256450755506\n",
      "0.256449992497\n",
      "0.256449229992\n",
      "0.256448467991\n",
      "0.256447706493\n",
      "0.256446945499\n",
      "0.256446185008\n",
      "0.256445425019\n",
      "0.256444665533\n",
      "0.256443906547\n",
      "0.256443148063\n",
      "0.256442390079\n",
      "0.256441632596\n",
      "0.256440875613\n",
      "0.256440119128\n",
      "0.256439363143\n",
      "0.256438607657\n",
      "0.256437852668\n",
      "0.256437098177\n",
      "0.256436344184\n",
      "0.256435590687\n",
      "0.256434837686\n",
      "0.256434085182\n",
      "0.256433333173\n",
      "0.256432581659\n",
      "0.25643183064\n",
      "0.256431080115\n",
      "0.256430330085\n",
      "0.256429580547\n",
      "0.256428831503\n",
      "0.256428082951\n",
      "0.256427334891\n",
      "0.256426587323\n",
      "0.256425840246\n",
      "0.25642509366\n",
      "0.256424347565\n",
      "0.25642360196\n",
      "0.256422856844\n",
      "0.256422112218\n",
      "0.25642136808\n",
      "0.256420624431\n",
      "0.25641988127\n",
      "0.256419138596\n",
      "0.256418396409\n",
      "0.256417654709\n",
      "0.256416913496\n",
      "0.256416172768\n",
      "0.256415432526\n",
      "0.256414692769\n",
      "0.256413953496\n",
      "0.256413214708\n",
      "0.256412476403\n",
      "0.256411738582\n",
      "0.256411001244\n",
      "0.256410264388\n",
      "0.256409528015\n",
      "0.256408792123\n",
      "0.256408056713\n",
      "0.256407321783\n",
      "0.256406587334\n",
      "0.256405853365\n",
      "0.256405119876\n",
      "0.256404386866\n",
      "0.256403654335\n",
      "0.256402922283\n",
      "0.256402190708\n",
      "0.256401459611\n",
      "0.256400728991\n",
      "0.256399998848\n",
      "0.256399269182\n",
      "0.256398539991\n",
      "0.256397811277\n",
      "0.256397083037\n",
      "0.256396355272\n",
      "0.256395627981\n",
      "0.256394901165\n",
      "0.256394174822\n",
      "0.256393448952\n",
      "0.256392723555\n",
      "0.25639199863\n",
      "0.256391274177\n",
      "0.256390550196\n",
      "0.256389826686\n",
      "0.256389103647\n",
      "0.256388381078\n",
      "0.256387658979\n",
      "0.256386937349\n",
      "0.256386216189\n",
      "0.256385495497\n",
      "0.256384775274\n",
      "0.256384055518\n",
      "0.256383336231\n",
      "0.25638261741\n",
      "0.256381899056\n",
      "0.256381181168\n",
      "0.256380463747\n",
      "0.256379746791\n",
      "0.2563790303\n",
      "0.256378314273\n",
      "0.256377598712\n",
      "0.256376883614\n",
      "0.256376168979\n",
      "0.256375454808\n",
      "0.2563747411\n",
      "0.256374027854\n",
      "0.25637331507\n",
      "0.256372602747\n",
      "0.256371890886\n",
      "0.256371179486\n",
      "0.256370468546\n",
      "0.256369758066\n",
      "0.256369048045\n",
      "0.256368338484\n",
      "0.256367629382\n",
      "0.256366920738\n",
      "0.256366212552\n",
      "0.256365504824\n",
      "0.256364797553\n",
      "0.25636409074\n",
      "0.256363384382\n",
      "0.256362678481\n",
      "0.256361973035\n",
      "0.256361268045\n",
      "0.25636056351\n",
      "0.256359859429\n",
      "0.256359155803\n",
      "0.25635845263\n",
      "0.256357749911\n",
      "0.256357047644\n",
      "0.256356345831\n",
      "0.256355644469\n",
      "0.25635494356\n",
      "0.256354243102\n",
      "0.256353543095\n",
      "0.256352843538\n",
      "0.256352144433\n",
      "0.256351445777\n",
      "0.25635074757\n",
      "0.256350049813\n",
      "0.256349352504\n",
      "0.256348655644\n",
      "0.256347959233\n",
      "0.256347263268\n",
      "0.256346567751\n",
      "0.256345872681\n",
      "0.256345178057\n",
      "0.25634448388\n",
      "0.256343790148\n",
      "0.256343096862\n",
      "0.256342404021\n",
      "0.256341711624\n",
      "0.256341019672\n",
      "0.256340328163\n",
      "0.256339637098\n",
      "0.256338946476\n",
      "0.256338256297\n",
      "0.25633756656\n",
      "0.256336877265\n",
      "0.256336188412\n",
      "0.2563355\n",
      "0.256334812029\n",
      "0.256334124498\n",
      "0.256333437407\n",
      "0.256332750757\n",
      "0.256332064545\n",
      "0.256331378773\n",
      "0.256330693439\n",
      "0.256330008544\n",
      "0.256329324086\n",
      "0.256328640066\n",
      "0.256327956483\n",
      "0.256327273337\n",
      "0.256326590627\n",
      "0.256325908353\n",
      "0.256325226515\n",
      "0.256324545112\n",
      "0.256323864145\n",
      "0.256323183611\n",
      "0.256322503512\n",
      "0.256321823847\n",
      "0.256321144615\n",
      "0.256320465816\n",
      "0.25631978745\n",
      "0.256319109517\n",
      "0.256318432015\n",
      "0.256317754945\n",
      "0.256317078306\n",
      "0.256316402099\n",
      "0.256315726321\n",
      "0.256315050974\n",
      "0.256314376057\n",
      "0.256313701569\n",
      "0.25631302751\n",
      "0.25631235388\n",
      "0.256311680678\n",
      "0.256311007905\n",
      "0.256310335559\n",
      "0.25630966364\n",
      "0.256308992148\n",
      "0.256308321082\n",
      "0.256307650443\n",
      "0.25630698023\n",
      "0.256306310442\n",
      "0.256305641079\n",
      "0.256304972141\n",
      "0.256304303628\n",
      "0.256303635538\n",
      "0.256302967872\n",
      "0.25630230063\n",
      "0.25630163381\n",
      "0.256300967413\n",
      "0.256300301438\n",
      "0.256299635886\n",
      "0.256298970754\n",
      "0.256298306044\n",
      "0.256297641755\n",
      "0.256296977886\n",
      "0.256296314438\n",
      "0.256295651409\n",
      "0.256294988799\n",
      "0.256294326609\n",
      "0.256293664837\n",
      "0.256293003484\n",
      "0.256292342548\n",
      "0.256291682031\n",
      "0.25629102193\n",
      "0.256290362247\n",
      "0.25628970298\n",
      "0.256289044129\n",
      "0.256288385695\n",
      "0.256287727676\n",
      "0.256287070072\n",
      "0.256286412883\n",
      "0.256285756108\n",
      "0.256285099748\n",
      "0.256284443801\n",
      "0.256283788268\n",
      "0.256283133148\n",
      "0.25628247844\n",
      "0.256281824146\n",
      "0.256281170263\n",
      "0.256280516792\n",
      "0.256279863732\n",
      "0.256279211083\n",
      "0.256278558845\n",
      "0.256277907018\n",
      "0.2562772556\n",
      "0.256276604592\n",
      "0.256275953994\n",
      "0.256275303804\n",
      "0.256274654023\n",
      "0.25627400465\n",
      "0.256273355685\n",
      "0.256272707128\n",
      "0.256272058978\n",
      "0.256271411235\n",
      "0.256270763899\n",
      "0.256270116969\n",
      "0.256269470444\n",
      "0.256268824325\n",
      "0.256268178612\n",
      "0.256267533303\n",
      "0.256266888399\n",
      "0.256266243899\n",
      "0.256265599803\n",
      "0.256264956111\n",
      "0.256264312821\n",
      "0.256263669935\n",
      "0.256263027451\n",
      "0.256262385369\n",
      "0.256261743689\n",
      "0.25626110241\n",
      "0.256260461533\n",
      "0.256259821057\n",
      "0.256259180981\n",
      "0.256258541305\n",
      "0.256257902029\n",
      "0.256257263152\n",
      "0.256256624675\n",
      "0.256255986597\n",
      "0.256255348917\n",
      "0.256254711635\n",
      "0.256254074751\n",
      "0.256253438264\n",
      "0.256252802175\n",
      "0.256252166482\n",
      "0.256251531186\n",
      "0.256250896286\n",
      "0.256250261782\n",
      "0.256249627674\n",
      "0.256248993961\n",
      "0.256248360642\n",
      "0.256247727718\n",
      "0.256247095188\n",
      "0.256246463053\n",
      "0.25624583131\n",
      "0.256245199961\n",
      "0.256244569005\n",
      "0.256243938441\n",
      "0.25624330827\n",
      "0.25624267849\n",
      "0.256242049102\n",
      "0.256241420106\n",
      "0.2562407915\n",
      "0.256240163285\n",
      "0.25623953546\n",
      "0.256238908025\n",
      "0.256238280979\n",
      "0.256237654323\n",
      "0.256237028056\n",
      "0.256236402178\n",
      "0.256235776688\n",
      "0.256235151586\n",
      "0.256234526872\n",
      "0.256233902545\n",
      "0.256233278605\n",
      "0.256232655051\n",
      "0.256232031885\n",
      "0.256231409104\n",
      "0.256230786709\n",
      "0.256230164699\n",
      "0.256229543075\n",
      "0.256228921835\n",
      "0.25622830098\n",
      "0.256227680509\n",
      "0.256227060422\n",
      "0.256226440718\n",
      "0.256225821398\n",
      "0.256225202461\n",
      "0.256224583906\n",
      "0.256223965733\n",
      "0.256223347942\n",
      "0.256222730533\n",
      "0.256222113505\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.3\n",
    "MSE_threshold = 0.05\n",
    "num_epochs = 2000\n",
    "\n",
    "train3LayerNetwork(input_patterns, output_patterns_XOR, learning_rate, MSE_threshold, num_epochs, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
